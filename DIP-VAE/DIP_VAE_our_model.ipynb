{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIP-VAE_our_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ameVGjEqtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "11097451-d21b-47c3-f482-ebef80a85964"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDrzf0nSE4En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12608d27-553a-4c71-8911-90ead5004a28"
      },
      "source": [
        "!git clone https://github.com/IBM/AIX360\n",
        "!pip install ./AIX360/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AIX360'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 1348 (delta 15), reused 9 (delta 2), pack-reused 1304\u001b[K\n",
            "Receiving objects: 100% (1348/1348), 357.23 MiB | 13.16 MiB/s, done.\n",
            "Resolving deltas: 100% (747/747), done.\n",
            "Processing ./AIX360\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (0.15.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (0.22.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (0.6.1+cu101)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.0.31)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.2.5)\n",
            "Requirement already satisfied: Image in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.5.32)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (2.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (1.4.1)\n",
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 41kB/s \n",
            "\u001b[?25hCollecting xport\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/a0/ade37253fe2c7a457a9a8703e93e4b1517dd53315e3941416ee4f7463f08/xport-2.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (0.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (2.23.0)\n",
            "Collecting xgboost==1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/91/551d37ba472bcbd70a25e667acc65a18a9d053657b13afcf0f87aa24d7bb/xgboost-1.0.2-py3-none-manylinux1_x86_64.whl (109.7MB)\n",
            "\u001b[K     |████████████████████████████████| 109.8MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: bleach>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (3.1.5)\n",
            "Requirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (0.15.2)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from aix360==0.2.0) (2.1.3)\n",
            "Collecting qpsolvers\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/1d/094bd9defdf99e69e45108a11d5e575a305b26f3286a9305c141abbf7bb9/qpsolvers-1.3.1.tar.gz\n",
            "Collecting lime==0.1.1.37\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/e0/60070b461a589b2fee0dbc45df9987f150fca83667c2f8a064cef7dbac6b/lime-0.1.1.37.tar.gz (275kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 42.3MB/s \n",
            "\u001b[?25hCollecting shap==0.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/db/58740847c0df6b346a999e3c375936436f4155354f6b644aa6e203bb40f3/shap-0.34.0.tar.gz (264kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->aix360==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->aix360==0.2.0) (7.0.0)\n",
            "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.6/dist-packages (from cvxpy->aix360==0.2.0) (2.1.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from cvxpy->aix360==0.2.0) (0.70.10)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from cvxpy->aix360==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.6/dist-packages (from cvxpy->aix360==0.2.0) (2.0.7.post1)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from Image->aix360==0.2.0) (3.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from Image->aix360==0.2.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->aix360==0.2.0) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->aix360==0.2.0) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->aix360==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->aix360==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aix360==0.2.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aix360==0.2.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aix360==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aix360==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->aix360==0.2.0) (2018.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (1.29.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (3.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 32.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14->aix360==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->aix360==0.2.0) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->aix360==0.2.0) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->aix360==0.2.0) (2.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->aix360==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->aix360==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->aix360==0.2.0) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->aix360==0.2.0) (2.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach>=2.1.0->aix360==0.2.0) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach>=2.1.0->aix360==0.2.0) (0.5.1)\n",
            "Collecting quadprog\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/f0/d4c8866f5d14dfa1a441439f5ce0d2680844651772129c431e78caadfe10/quadprog-0.1.7.tar.gz\n",
            "Collecting progressbar\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap==0.34.0->aix360==0.2.0) (4.41.1)\n",
            "Requirement already satisfied: dill>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from multiprocess->cvxpy->aix360==0.2.0) (0.3.2)\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from django->Image->aix360==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: asgiref~=3.2 in /usr/local/lib/python3.6/dist-packages (from django->Image->aix360==0.2.0) (3.2.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14->aix360==0.2.0) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14->aix360==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14->aix360==0.2.0) (3.2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->aix360==0.2.0) (4.4.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from quadprog->qpsolvers->aix360==0.2.0) (0.29.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14->aix360==0.2.0) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14->aix360==0.2.0) (3.1.0)\n",
            "Building wheels for collected packages: aix360, qpsolvers, lime, shap, quadprog, progressbar\n",
            "  Building wheel for aix360 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aix360: filename=aix360-0.2.0-cp36-none-any.whl size=58397341 sha256=71ea80670b946ba2c63d4034e3667df94d2b02b2bb12f3d64688d5b3487d6828\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r669f1st/wheels/e1/45/37/749bf7f76b28af881e06780e87624f8cb542217ac07c670e82\n",
            "  Building wheel for qpsolvers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qpsolvers: filename=qpsolvers-1.3.1-cp36-none-any.whl size=17278 sha256=17a0f0d8a7ca7c9f370e28ec7759acc95d35209fa5dfecc63217c1bf703bfbe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/ae/47/cd1e348d6077b49e6632bcde233f03b9765555a6201f6e8a64\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.1.1.37-cp36-none-any.whl size=284277 sha256=120c0537d7125f652b596a5b8997e9bf9f101f286a5ab4cd6cf13f430a2ad1a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/38/e7/50d75d4fb75afa604570dc42f20c5c5f5ab26d3fbe8d6ef27b\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.34.0-cp36-cp36m-linux_x86_64.whl size=383175 sha256=df6945d53fca9818619ae738311d6e72b51a8c342234928569aa0f417f36ebc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/7e/28/57bc9fcb77579b0df6561298648249370648c0b91dfe42c8ec\n",
            "  Building wheel for quadprog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for quadprog: filename=quadprog-0.1.7-cp36-cp36m-linux_x86_64.whl size=299705 sha256=f2d188cb90239f5da767abc16d64a05d6dadad021419400f21a252a0a4880867\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/dd/b1/849989444c0a5930927b260663019b7da6cff864fc224c2747\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=22bab4b94b44bf420ed07c98a7672623bff8decc9ad1d217ad2dc449a514f0db\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
            "Successfully built aix360 qpsolvers lime shap quadprog progressbar\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, xport, xgboost, quadprog, qpsolvers, progressbar, lime, shap, aix360\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed aix360-0.2.0 lime-0.1.1.37 progressbar-2.5 qpsolvers-1.3.1 quadprog-0.1.7 shap-0.34.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 xgboost-1.0.2 xport-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tor-Nc1E7re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"../../\")\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from aix360.algorithms.dipvae.dipvae_utils import VAE, DIPVAE\n",
        "from aix360.algorithms.dipvae import DIPVAEExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuGiLI6PE_i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "332936af-1eff-4885-a2b7-c4c8522b8bdb"
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "print(\"CUDA: {}\".format(cuda_available))\n",
        "\n",
        "base_path = '/content/drive/My Drive/Datascience1/own-model/'\n",
        "print(base_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "/content/drive/My Drive/Datascience1/own-model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEK7110Xtzpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from aix360.algorithms.dipvae.dipvae_utils import VAE, DIPVAE\n",
        "from aix360.algorithms.dipvae import DIPVAEExplainer\n",
        "from aix360.algorithms.dipvae.dipvae_utils import plot_reconstructions, plot_latent_traversal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETZsH_BCHKm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "61d1f4fd-32c3-4634-f736-f12c9a48f93f"
      },
      "source": [
        "class FlowersImages(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, file_path, transform=None):\n",
        "        self.root = root\n",
        "        self.data  = pd.read_csv(file_path)\n",
        "\n",
        "        print(self.data)\n",
        "\n",
        "        self.data['disease_class'] = self.data.apply(lambda x: self.data.columns[x==1], axis = 1)\n",
        "        self.data['disease_class'] = self.data['disease_class'].apply(lambda x: str(x[0]))\n",
        "\n",
        "        diseases = {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'nv2': 6, 'nv3': 7, 'vasc': 8}\n",
        "        self.data['disease_class_int'] = self.data['disease_class'].map(diseases)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.root + self.data.iloc[index, 1]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        \n",
        "        label = self.data.iloc[index, 10]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "            #imghsv = self.transform(imghsv)\n",
        "\n",
        "        #img_6channel = np.vstack((img, imghsv))\n",
        "\n",
        "        return img, label #img_6channel\n",
        "\n",
        "\n",
        "class FlowersDataset():\n",
        "    def __init__(self, batch_size=256,\n",
        "                 root_images_path=\"/content/drive/My Drive/Dataset_skin_cancer\",\n",
        "                 file_path_labels='/content/drive/My Drive/Datascience1/disease_classes.csv'):\n",
        "        img_size = (128, 128)  # (224,224)\n",
        "\n",
        "        # Transforms\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(size=img_size),\n",
        "            transforms.ToTensor()\n",
        "\n",
        "        ])\n",
        "\n",
        "        train_set = FlowersImages(root=root_images_path, file_path=file_path_labels, transform=transform)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset=train_set,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True)\n",
        "\n",
        "        self.name = \"flowers\"\n",
        "        self.data_dims = [3, 128, 128]\n",
        "        self.train_size = len(self.train_loader)\n",
        "        self.range = [-1.0, 1.0]\n",
        "        self.batch_size = batch_size\n",
        "        self.num_training_instances = len(train_set)\n",
        "        self.likelihood_type = \"gaussian\"\n",
        "        self.output_activation_type = \"tanh\"\n",
        "\n",
        "    def next_batch(self):\n",
        "        for x, y in self.train_loader:\n",
        "            x = 2.0 * (x - 0.5)\n",
        "            yield x, y\n",
        "\n",
        "class ConvEncNet(nn.Module):\n",
        "    def __init__(self, latent_dim=20, num_filters=64, num_channels=3, image_size=128, activation_type='relu', args=None):\n",
        "        super(ConvEncNet, self).__init__()\n",
        "        self.args = args\n",
        "        if activation_type == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation_type == 'tanh':\n",
        "            self.activation = nn.Tanh(inplace=True)\n",
        "        else:\n",
        "            print(\"Activation Type not supported\")\n",
        "            return\n",
        "\n",
        "        self.conv_hidden = []\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_filters, 4, 2, 1, bias=True)\n",
        "\n",
        "        num_layers = math.log2(image_size)\n",
        "        assert num_layers == round(num_layers), 'Image size that are power of 2 are supported.'\n",
        "        num_layers = int(num_layers)\n",
        "\n",
        "        for i in np.arange(num_layers - 3):\n",
        "            self.conv_hidden.append(nn.Conv2d(num_filters * 2 ** i, num_filters * 2 ** (i + 1), 4, 2, 1, bias=True))\n",
        "            self.conv_hidden.append(nn.BatchNorm2d(num_filters * 2 ** (i + 1)))\n",
        "            self.conv_hidden.append(self.activation)\n",
        "\n",
        "        self.features = nn.Sequential(*self.conv_hidden)\n",
        "        self.conv_mu = nn.Conv2d(num_filters * 2 ** (num_layers - 3), latent_dim, 4)\n",
        "        self.conv_var = nn.Conv2d(num_filters * 2 ** (num_layers - 3), latent_dim, 4)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.features(x)\n",
        "        mu_z = self.conv_mu(x).view(x.size(0), -1)\n",
        "        var_z = self.conv_var(x).view(x.size(0), -1)\n",
        "        return torch.cat([mu_z, var_z], dim=1)\n",
        "\n",
        "\n",
        "class ConvDecNet(nn.Module):\n",
        "    def __init__(self, latent_dim=20, num_filters=64, num_channels=3, image_size=128, activation_type='relu', args=None):\n",
        "        super(ConvDecNet, self).__init__()\n",
        "        self.args = args\n",
        "        if activation_type == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation_type == 'tanh':\n",
        "            self.activation = nn.Tanh(inplace=True)\n",
        "        else:\n",
        "            print(\"Activation Type not supported\")\n",
        "            return\n",
        "\n",
        "        self.deconv_hidden = []\n",
        "\n",
        "        num_layers = math.log2(image_size)\n",
        "        assert num_layers == round(num_layers), 'Image size that are power of 2 are supported.'\n",
        "        num_layers = int(num_layers)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(latent_dim, num_filters * 2 ** (num_layers - 3), 4, 1, 0, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(num_filters * 2 ** (num_layers - 3))\n",
        "\n",
        "        for i in np.arange(num_layers - 3, 0, -1):\n",
        "            self.deconv_hidden.append(nn.ConvTranspose2d(num_filters * 2 ** i,\n",
        "                                                         num_filters * 2 ** (i - 1),\n",
        "                                                         4, 2, 1, bias=True))\n",
        "            self.deconv_hidden.append(nn.BatchNorm2d(num_filters * 2 ** (i - 1)))\n",
        "            self.deconv_hidden.append(self.activation)\n",
        "\n",
        "        self.features = nn.Sequential(*self.deconv_hidden)\n",
        "\n",
        "        self.deconv_out = nn.ConvTranspose2d(num_filters, num_channels, 4, 2,1, bias=True)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.deconv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.features(x)\n",
        "        return self.deconv_out(x)\n",
        "\n",
        "\n",
        "class ConvDIPVAE(DIPVAE):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, num_filters=50, latent_dim=10, num_channels=3, image_size=128, activation_type='relu',\n",
        "                 args=None, cuda_available=None, init_networks=True, mode=None, output_activation_type='tanh',\n",
        "                 likelihood_type=None, beta=1.0):\n",
        "        super(ConvDIPVAE, self).__init__(latent_dim=latent_dim,\n",
        "                                     activation_type=activation_type,\n",
        "                                     args=args, cuda_available=cuda_available, init_networks=False, mode=mode,\n",
        "                                     output_activation_type=output_activation_type,likelihood_type=likelihood_type,\n",
        "                                         beta=beta)\n",
        "\n",
        "        if init_networks:\n",
        "            self.generative_net = ConvDecNet(latent_dim=latent_dim, num_filters=num_filters, num_channels=num_channels,\n",
        "                                             image_size=image_size,activation_type=activation_type, args=args)\n",
        "            self.inference_net = ConvEncNet(latent_dim=latent_dim, num_filters=num_filters, num_channels=num_channels,\n",
        "                                            image_size=image_size, activation_type=activation_type, args=args)\n",
        "\n",
        "        self.name = \"ConvDIPVAE\"\n",
        "\n",
        "\n",
        "dataset_obj = FlowersDataset(root_images_path=\"/content/drive/My Drive/Dataset_skin_cancer/\",\n",
        "                          file_path_labels=\"/content/drive/My Drive/Datascience1/disease_classes.csv\",\n",
        "                          batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         0  ISIC_0031609.jpg  1.0  0.0  ...  0.0.4  0.0.5  0.0.6  0.0.7\n",
            "0        1  ISIC_0029659.jpg  1.0  0.0  ...    0.0    0.0    0.0    0.0\n",
            "1        2  ISIC_0027802.jpg  1.0  0.0  ...    0.0    0.0    0.0    0.0\n",
            "2        3  ISIC_0028335.jpg  1.0  0.0  ...    0.0    0.0    0.0    0.0\n",
            "3        4  ISIC_0028763.jpg  1.0  0.0  ...    0.0    0.0    0.0    0.0\n",
            "4        5  ISIC_0026702.jpg  1.0  0.0  ...    0.0    0.0    0.0    0.0\n",
            "...    ...               ...  ...  ...  ...    ...    ...    ...    ...\n",
            "10174  509  ISIC_0026766.jpg  0.0  1.0  ...    0.0    0.0    0.0    0.0\n",
            "10175  510  ISIC_0027865.jpg  0.0  1.0  ...    0.0    0.0    0.0    0.0\n",
            "10176  511  ISIC_0027851.jpg  0.0  1.0  ...    0.0    0.0    0.0    0.0\n",
            "10177  512  ISIC_0026091.jpg  0.0  1.0  ...    0.0    0.0    0.0    0.0\n",
            "10178  513  ISIC_0030230.jpg  0.0  1.0  ...    0.0    0.0    0.0    0.0\n",
            "\n",
            "[10179 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLW4qJpdMM_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dipvaeii_model_args = argparse.Namespace()\n",
        "\n",
        "dipvaeii_model_args.model = 'user-defined'\n",
        "dipvaeii_model_args.activation_type = 'relu'\n",
        "dipvaeii_model_args.num_filters = 32\n",
        "dipvaeii_model_args.latent_dim = 10\n",
        "dipvaeii_model_args.num_channels =  dataset_obj.data_dims[0]\n",
        "dipvaeii_model_args.image_size =  dataset_obj.data_dims[-1]\n",
        "dipvaeii_model_args.step_size = 0.0001\n",
        "dipvaeii_model_args.num_epochs = 10\n",
        "dipvaeii_model_args.lambda_diag_factor = 10.0\n",
        "dipvaeii_model_args.lambda_offdiag = 0.001\n",
        "dipvaeii_model_args.output_activation_type = dataset_obj.output_activation_type\n",
        "dipvaeii_model_args.likelihood_type = dataset_obj.likelihood_type\n",
        "dipvaeii_model_args.mode='ii'\n",
        "dipvaeii_model_args.beta=0.001\n",
        "dipvaeii_model_args.seed=0\n",
        "\n",
        "dipvaeii_model_args.fit=1\n",
        "dipvaeii_model_args.root_save_dir = base_path\n",
        "\n",
        "# Baseline VAE where lambda_diag_factor and lambda_offdiag are set to 0.0\n",
        "\n",
        "vae_model_args = argparse.Namespace()\n",
        "\n",
        "vae_model_args.model = 'user-defined'\n",
        "vae_model_args.activation_type = 'relu'\n",
        "vae_model_args.num_filters = 32\n",
        "vae_model_args.latent_dim = 10\n",
        "vae_model_args.num_channels =  dataset_obj.data_dims[0]\n",
        "vae_model_args.image_size =  dataset_obj.data_dims[-1]\n",
        "vae_model_args.step_size = 0.0001\n",
        "vae_model_args.num_epochs = 80\n",
        "vae_model_args.lambda_diag_factor = 0.0\n",
        "vae_model_args.lambda_offdiag = 0.0\n",
        "vae_model_args.output_activation_type = dataset_obj.output_activation_type\n",
        "vae_model_args.likelihood_type = dataset_obj.likelihood_type\n",
        "vae_model_args.mode='ii'\n",
        "vae_model_args.beta=0.001\n",
        "vae_model_args.seed=0\n",
        "\n",
        "vae_model_args.fit=1\n",
        "vae_model_args.root_save_dir = base_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Zzu8U1Miw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "7f2451c3-5cea-4916-adc3-86ee3d92c8c1"
      },
      "source": [
        "\n",
        "# Train model\n",
        "model_args = dipvaeii_model_args\n",
        "\n",
        "setup = [\n",
        "    ('model={:s}', model_args.model),\n",
        "    ('lambda_diag_factor={:.0e}', model_args.lambda_diag_factor),\n",
        "    ('lambda_offdiag={:.0e}', model_args.lambda_offdiag),\n",
        "    ('beta={:.0e}', model_args.beta),\n",
        "]\n",
        "save_dir = os.path.join(model_args.root_save_dir, \"results\"+'_'.join([t.format(v) for (t, v) in setup]))\n",
        "\n",
        "\n",
        "if model_args.fit:\n",
        "    net = ConvDIPVAE(num_filters=model_args.num_filters, latent_dim=model_args.latent_dim,\n",
        "                  num_channels=model_args.num_channels, image_size=model_args.image_size,\n",
        "                  activation_type=model_args.activation_type, args=model_args, cuda_available=cuda_available,\n",
        "                  init_networks=True, mode=model_args.mode, output_activation_type=model_args.output_activation_type,\n",
        "                  likelihood_type=model_args.likelihood_type, beta=model_args.beta)\n",
        "else:\n",
        "    net = torch.load(os.path.join(save_dir, net))\n",
        "\n",
        "dipvaeii_explainer = DIPVAEExplainer(net=net, dataset=dataset_obj, cuda_available=cuda_available,\n",
        "                                      model_args=model_args)\n",
        "\n",
        "if model_args.fit:\n",
        "    loss_epoch_list = dipvaeii_explainer.fit(visualize=True, save_dir=save_dir)\n",
        "    torch.save(net, os.path.join(save_dir, net))\n",
        "\n",
        "# After training\n",
        "for x, _ in dataset_obj.next_batch():\n",
        "    if dipvaeii_explainer.cuda_available:\n",
        "        x = x.cuda()\n",
        "    plot_reconstructions(dipvaeii_explainer.dataset, dipvaeii_explainer.net, x, image_id_to_plot=2,\n",
        "                          epoch='end', batch_id = 'end', save_dir=save_dir)\n",
        "    plot_latent_traversal(dipvaeii_explainer, x, dipvaeii_explainer.model_args, dipvaeii_explainer.dataset,\n",
        "                          image_id_to_plot=2, epoch='end', batch_id='end', save_dir=save_dir)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-63dc2200d52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss_epoch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdipvaeii_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/aix360/algorithms/dipvae/dipvae.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, visualize, save_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;31m#x, y = torch.tensor(x), torch.tensor(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c5faf555f2cb>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c5faf555f2cb>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Dataset_skin_cancer/.DS_Store'"
          ]
        }
      ]
    }
  ]
}